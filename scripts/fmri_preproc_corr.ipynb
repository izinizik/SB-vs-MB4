{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a02c0e-fbe8-4212-a4e8-98b6f1da305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "from nilearn.maskers import NiftiLabelsMasker\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from nipype.interfaces import afni \n",
    "from joblib import Parallel, delayed\n",
    "import logging\n",
    "import uuid\n",
    "\n",
    "os.environ[\"PATH\"] += os.pathsep + os.path.expanduser(\"~/abin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93fe525-f625-404e-9cf1-27cbbbdd4d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to data\n",
    "data_path = \"/Users/labneuro2/Documents/lab/SBvsMB4/halfpipe\"\n",
    "\n",
    "# Regressors to remove\n",
    "REGRESSORS_TO_USE = [\n",
    "    \"trans_x\", \"trans_y\", \"trans_z\", \"rot_x\", \"rot_y\", \"rot_z\",\n",
    "    \"a_comp_cor_00\", \"a_comp_cor_01\", \"a_comp_cor_02\", \"a_comp_cor_03\", \"a_comp_cor_04\"\n",
    "]\n",
    "\n",
    "# Bandpass filters and their corresponding folder names\n",
    "FILTER_SETTINGS = {\"high_pass\": 0.008, \"low_pass\": 0.09, \"label\": \"bp_008_090\"}\n",
    "\n",
    "FD_THRESHOLDS = [0.4, 1.0]\n",
    "\n",
    "# Paths to mask and ROI atlas\n",
    "atlases_path = \"/Users/labneuro2/Documents/lab/SBvsMB4/atlases\"\n",
    "parcels = [\"100Parcels\", \"200Parcels\", \"400Parcels\"]\n",
    "tians = [\"S1\", \"S2\", \"S4\"]\n",
    "atlas_filenames, output_paths = [], []\n",
    "for parc, tian in zip(parcels, tians):\n",
    "    atlas_filenames.append(f\"{atlases_path}/Schaefer2018_{parc}_7Networks_order_Tian_Subcortex_{tian}_3T_MNI152NLin2009cAsym_2mm.nii.gz\")\n",
    "    atlas_name = f\"schaefer_{parc}_Tian_{tian}\"\n",
    "    output_paths.append(f\"/Users/labneuro2/Documents/lab/SBvsMB4/correlation_matrices/{atlas_name}\")\n",
    "\n",
    "\n",
    "# Find all NIfTI files\n",
    "nifti_files = sorted(glob.glob(f\"{data_path}/**/*preproc_bold.nii.gz\", recursive=True))\n",
    "\n",
    "# Temporary directory for intermediate files\n",
    "temp_dir = \"/Users/labneuro2/Documents/lab/SBvsMB4/temp_files\"\n",
    "os.makedirs(temp_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29731732-1ecd-48c3-afb1-47a0b7440cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_correlation_min(base_nifti_file, fmri_file, confound_file, high_pass, low_pass,\n",
    "                              mask_file, atlas_filenames, output_paths, confound_vars,\n",
    "                              temp_id, minutes, fd_threshold, cenmode='KILL'):\n",
    "\n",
    "    import re\n",
    "    logging.getLogger(\"nipype\").setLevel(logging.CRITICAL)\n",
    "\n",
    "    img = nib.load(fmri_file)\n",
    "    tr = img.header.get_zooms()[3]\n",
    "\n",
    "    # Load confounds and select valid columns\n",
    "    confounds_df = pd.read_csv(confound_file, sep='\\t')\n",
    "    valid_confounds = [col for col in confound_vars if col in confounds_df.columns]\n",
    "    confounds = confounds_df[valid_confounds].fillna(0).values\n",
    "\n",
    "    # Save confounds to .1D file for AFNI\n",
    "    confounds_1d_path = f\"{temp_dir}/motion_regressors_{temp_id}.1D\"\n",
    "    np.savetxt(confounds_1d_path, confounds, fmt=\"%.6f\")\n",
    "\n",
    "    # Create censor file based on framewise displacement threshold\n",
    "    censor_1d_path = f\"{temp_dir}/censor_{temp_id}.1D\"\n",
    "    if \"framewise_displacement\" in confounds_df.columns:\n",
    "        censor_series = (confounds_df[\"framewise_displacement\"] < fd_threshold).astype(int)\n",
    "    else:\n",
    "        censor_series = np.ones(len(confounds_df))\n",
    "    np.savetxt(censor_1d_path, censor_series.values, fmt=\"%d\")\n",
    "\n",
    "    # Set up AFNI TProject for temporal filtering and nuisance regression\n",
    "    nifti_output_afni = f\"{temp_dir}/filtered_fmri_{temp_id}.nii.gz\"\n",
    "    tproject = afni.TProject()\n",
    "    tproject.inputs.in_file = fmri_file\n",
    "    tproject.inputs.out_file = nifti_output_afni\n",
    "    tproject.inputs.bandpass = (high_pass, low_pass)\n",
    "    tproject.inputs.polort = 1\n",
    "    tproject.inputs.ort = confounds_1d_path\n",
    "    tproject.inputs.mask = mask_file\n",
    "    tproject.inputs.censor = censor_1d_path\n",
    "    tproject.inputs.cenmode = cenmode\n",
    "\n",
    "    try:\n",
    "        print(f\"🖥️ AFNI 3dTproject CMD: {tproject.cmdline}\")\n",
    "        res = tproject.run()\n",
    "        stderr = res.runtime.stderr\n",
    "\n",
    "        # Parse degrees of freedom (DOF) from AFNI output\n",
    "        dof_match = re.search(r\"==>\\s+(\\d+)\\s+D\\.O\\.F\\. left\", stderr)\n",
    "        if dof_match:\n",
    "            afni_dof = int(dof_match.group(1))\n",
    "            print(f\"DOF from AFNI: {afni_dof}\")\n",
    "        else:\n",
    "            afni_dof = None\n",
    "            print(\"Could not extract DOF from AFNI stderr.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error running AFNI 3dTproject: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    # Load filtered data\n",
    "    img_filtered = nib.load(nifti_output_afni)\n",
    "\n",
    "    for atlas_filename, output_path in zip(atlas_filenames, output_paths):\n",
    "        # Extract time series using atlas\n",
    "        masker = NiftiLabelsMasker(labels_img=atlas_filename, mask_img=mask_file, standardize=True, t_r=tr)\n",
    "        time_series = masker.fit_transform(img_filtered)\n",
    "\n",
    "        # Compute correlation matrix\n",
    "        connectivity_measure = ConnectivityMeasure(kind='correlation')\n",
    "        correlation_matrix = connectivity_measure.fit_transform([time_series])[0]\n",
    "\n",
    "        # Save correlation matrix to CSV\n",
    "        new_filename_csv = base_nifti_file.split('/')[-1].replace(\n",
    "            \"setting-preproc_bold.nii.gz\",\n",
    "            f\"trimmed_{minutes}min_correlation.csv\"\n",
    "        )\n",
    "        output_file_csv = os.path.join(output_path, new_filename_csv)\n",
    "        os.makedirs(os.path.dirname(output_file_csv), exist_ok=True)\n",
    "        np.savetxt(output_file_csv, correlation_matrix, delimiter=\",\")\n",
    "\n",
    "    # Clean up temporary files\n",
    "    for f in [nifti_output_afni, confounds_1d_path, censor_1d_path]:\n",
    "        if os.path.exists(f):\n",
    "            os.remove(f)\n",
    "\n",
    "    return correlation_matrix, afni_dof\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5e4042-be42-4d1e-b871-ea55ba2f98b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_nifti(nifti_file, temp_id):\n",
    "    confounds_file = nifti_file.replace(\"bold.nii.gz\", \"desc-confounds_regressors.tsv\")\n",
    "    mask_file = nifti_file.replace(\"bold\", \"desc-brain_mask\")\n",
    "    dof_records = []\n",
    "    img = nib.load(nifti_file)\n",
    "    data = img.get_fdata()\n",
    "    affine = img.affine\n",
    "    header = img.header\n",
    "    tr = header.get_zooms()[3]\n",
    "    total_volumes = data.shape[3]\n",
    "\n",
    "    confounds_df = pd.read_csv(confounds_file, sep='\\t', na_values='n/a')\n",
    "\n",
    "    for minutes in range(5, 14):\n",
    "        target_volumes = int(minutes * 60 / tr)\n",
    "        if target_volumes > total_volumes:\n",
    "            target_volumes = total_volumes\n",
    "\n",
    "        trimmed_data = data[:, :, :, :target_volumes]\n",
    "        trimmed_img = nib.Nifti1Image(trimmed_data, affine, header)\n",
    "        trimmed_confounds = confounds_df.iloc[:target_volumes]\n",
    "\n",
    "        for fd_threshold in FD_THRESHOLDS:\n",
    "            for setting in FILTER_SETTINGS:\n",
    "                fd_label = f\"fd_{int(fd_threshold * 1000):03d}\"\n",
    "                setting_label = f\"{FILTER_SETTINGS['label']}_{fd_label}\"\n",
    "                unique_id = uuid.uuid4().hex[:6]\n",
    "                temp_suffix = f\"{temp_id}_{minutes}_{setting_label}\"\n",
    "\n",
    "                # Temporary files for trimmed fMRI and confounds\n",
    "                trimmed_fmri_file = f\"{temp_dir}/trimmed_fmri_{temp_suffix}.nii.gz\"\n",
    "                trimmed_confounds_filename = f\"{temp_dir}/trimmed_confounds_{temp_suffix}.tsv\"\n",
    "\n",
    "                nib.save(trimmed_img, trimmed_fmri_file)\n",
    "                trimmed_confounds.to_csv(trimmed_confounds_filename, sep='\\t', index=False)\n",
    "\n",
    "                # Create output directories for each atlas\n",
    "                for path in output_paths:\n",
    "                    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "                correlation_matrix, afni_dof = calculate_correlation_min(\n",
    "                    nifti_file,\n",
    "                    trimmed_fmri_file, trimmed_confounds_filename,\n",
    "                    FILTER_SETTINGS[\"high_pass\"], FILTER_SETTINGS[\"low_pass\"],\n",
    "                    mask_file, atlas_filenames, output_paths_setting,\n",
    "                    REGRESSORS_TO_USE,\n",
    "                    temp_suffix, minutes,\n",
    "                    fd_threshold=fd_threshold\n",
    "                )\n",
    "\n",
    "                dof_records.append({\n",
    "                    \"file\": os.path.basename(nifti_file),\n",
    "                    \"minutes\": minutes,\n",
    "                    \"volumes\": target_volumes,\n",
    "                    \"fd_threshold\": fd_threshold,\n",
    "                    \"afni_dof\": afni_dof\n",
    "                })\n",
    "\n",
    "                # Clean up temporary files\n",
    "                for f in [trimmed_fmri_file, trimmed_confounds_filename]:\n",
    "                    if os.path.exists(f):\n",
    "                        os.remove(f)\n",
    "\n",
    "    return dof_records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eb55eb-508e-441f-bf3f-5775ded5d6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = Parallel(n_jobs=48)(\n",
    "    delayed(process_nifti)(nifti_file, i)\n",
    "    for i, nifti_file in enumerate(nifti_files)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ffaacd-8fe1-4a10-a22d-bccbcdb8e73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the list of results into a single list\n",
    "flattened_dof_records = [item for sublist in results for item in sublist]\n",
    "\n",
    "# Save the results to a CSV file\n",
    "dof_df = pd.DataFrame(flattened_dof_records)\n",
    "dof_df.to_csv(\"dof_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4592902-d978-4864-936a-afbeb5788135",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
